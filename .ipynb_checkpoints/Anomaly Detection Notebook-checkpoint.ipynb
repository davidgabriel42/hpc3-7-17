{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.decomposition import PCA, FactorAnalysis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import os\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Trim Datasets\n",
    "##Loads data and trims tuples down to correct 2 hour time period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_dataset(path):\n",
    "    out_data = pd.DataFrame()\n",
    "    for sub_dir in os.listdir(path):\n",
    "        temp_path = os.path.join(path, sub_dir)\n",
    "        temp_path = os.path.join(temp_path, \"log_tcp_complete\")\n",
    "#        print(temp_path)\n",
    "        if os.path.isfile(temp_path): \n",
    "#            print('1')\n",
    "#            temp_data = read_in_file(temp_path)\n",
    "            temp_data = pd.read_csv(temp_path, delimiter= '\\s', index_col=False)\n",
    "            temp_data.shape\n",
    "            out_data = out_data.append(temp_data) \n",
    "        #np.concatenate((out_data, temp_data))\n",
    "#    return \n",
    "    df = out_data\n",
    "    #load data\n",
    "#def trim_dataset(df):\n",
    "\n",
    "    #create timestamp    \n",
    "    df['t_first']=df['first:29'].astype('float').astype(\"datetime64[ms]\")\n",
    "    df['t_last']=df['last:30'].astype('float').astype(\"datetime64[ms]\")\n",
    "\n",
    "    #find start/end time\n",
    "    t_start = df.t_first.min()\n",
    "    t_end = t_start+ pd.Timedelta(hours=2)\n",
    "    t_max = df.t_last.max()\n",
    "\n",
    "    #trim data after test complete (tstat ran too long in bash shell)    \n",
    "    df = df[(df['t_first'] >= t_start) & (df['t_last'] < t_end)]\n",
    "#    df = df.values\n",
    "\n",
    "    df.drop(columns=['t_first', 't_last'])\n",
    "    df.drop(df.index[0])\n",
    "\n",
    "#    csv = get_data_row(df.to_csv( index = False, sep = \" \"))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Label Indexes\n",
    "This chunk loads a file that contains the labels we want to load from the datasets as well as their indicies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open(\"tstat_labels_indexes.txt\" ,'r')\n",
    "data_field_list = []\n",
    "for line in infile.readlines():\n",
    "    if \":\" in line:\n",
    "        data_field = str(re.search('%s(.*)%s' % (\"\\\"\", \"\\\"\"), line).group(1))\n",
    "        index = int(re.search('%s(.*)%s' % (\":\", \",\"), line).group(1))\n",
    "        data_field_list.append((data_field, index))\n",
    "\n",
    "index_to_key_dict = {}\n",
    "key_to_index_dict = {}\n",
    "data_field_labels = []\n",
    "for data_field, index in data_field_list:\n",
    "    key_to_index_dict[data_field] = index\n",
    "    index_to_key_dict[index] = data_field\n",
    "    data_field_labels.append(data_field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in a dataset file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in_file(input):\n",
    "    entries = []\n",
    "    labels = None\n",
    "    for i, line in enumerate(input):\n",
    "        row = get_data_row(line)\n",
    "        row = clean_data_row(row)\n",
    "        if row != []:\n",
    "            entries.append(row)\n",
    "    entries = np.array(entries)\n",
    "    return entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data row\n",
    "Called by the read in file function. Loads a single line from the dataset files. Super inefficient, but only loads labels which are in the data field list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_row(line):\n",
    "    global index_to_key_dict\n",
    "#    print(line)\n",
    "    line = str(line).split(',')\n",
    "    row = []\n",
    "    labels = []\n",
    "    c_pkt_cnt = 0\n",
    "    s_pkt_cnt = 0\n",
    "    c_bytes_cnt = 0\n",
    "    s_bytes_cnt = 0\n",
    "    for data_field, index in data_field_list:\n",
    "#        print(line[index]) \n",
    "        if data_field == \"client_pkt_cnt\":\n",
    "            try:\n",
    "                c_pkt_cnt = line[index]\n",
    "                c_pkt_cnt = max(float(c_pkt_cnt), 1)\n",
    "            except:\n",
    "                c_pkt_cnt = 1\n",
    "            #if c_pkt_cnt < 32:\n",
    "            #    return []\n",
    "        elif data_field == \"serv_pkt_cnt\":\n",
    "            try:\n",
    "                s_pkt_cnt = line[index]\n",
    "                s_pkt_cnt = max(float(s_pkt_cnt), 1)\n",
    "            except:\n",
    "                s_pkt_cnt = 1\n",
    "        elif data_field == \"client_bytes_cnt\":\n",
    "            try:\n",
    "                c_bytes_cnt = line[index]\n",
    "                c_bytes_cnt = max(float(c_bytes_cnt), 1)\n",
    "            except:\n",
    "                c_bytes_cnt = 1\n",
    "        elif data_field == \"serv_bytes_cnt\":\n",
    "            try:\n",
    "                s_bytes_cnt = line[index]\n",
    "                s_bytes_cnt = max(float(s_bytes_cnt), 1)\n",
    "            except:\n",
    "                s_bytes_cnt = 1\n",
    "                \n",
    "    for data_field, index in data_field_list:\n",
    "        try:\n",
    "            val = line[index]\n",
    "            val = float(val)\n",
    "        except:\n",
    "            val = 0\n",
    "        if data_field in [\"client_pkt_cnt\", \"client_rst_cnt\", \"client_ack_cnt\", \"client_pkt_data\", \"client_pkt_retx\",\n",
    "                         \"client_syn_cnt\", \"client_fin_cnt\", \"client_pkt_retx\"]:\n",
    "            val /= c_pkt_cnt\n",
    "        elif data_field in [\"client_bytes_uniq\", \"client_bytes_cnt\", \"client_bytes_retx\"]:\n",
    "            val /= c_bytes_cnt\n",
    "        elif data_field in [\"serv_pkt_cnt\", \"serv_rst_cnt\", \"serv_ack_cnt\", \"serv_ack_pck_cnt\", \"serv_pkts_data\", \n",
    "                            \"serv_pkts_retx\", \"serv_syn_cnt\", \"serv_fin_cnt\"]:\n",
    "            val /= s_pkt_cnt\n",
    "        elif data_field in [\"serv_bytes_uniq\", \"serv_btyes_cnt\", \"serv_pkts_retx\"]:\n",
    "            val /= s_bytes_cnt\n",
    "        row.append(val)    \n",
    "    return row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean data row\n",
    "Not implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data_row(in_row):\n",
    "    global index_to_key_dict, key_to_index_dict\n",
    "    return in_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get dataset\n",
    "Loads all files from a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_dataset(path):\n",
    "    print(path)\n",
    "    temp_data = load_dataset(path)\n",
    "    temp_data = temp_data.drop(['t_first', 't_last'], axis=1)\n",
    "    csv = (temp_data.to_csv( index = False, sep = \" \"))\n",
    "    entries = []\n",
    "    for i, line in enumerate(csv.splitlines()):\n",
    "        row = get_data_row(line)\n",
    "        row = clean_data_row(row)\n",
    "        if row != []:\n",
    "            entries.append(row)\n",
    "    entries = np.array(entries)\n",
    "    \n",
    "    return entries\n",
    "\n",
    "    # add pandas to list and row algo\n",
    "\n",
    "#pd.DataFrame(temp_data)\n",
    "#print(\"output::\", csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./normal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dave/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:11: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0    1    2    3    4    5    6    7    8    9    10   11   12   13  \\\n",
      "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "2858  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "2859  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "2860  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "2861  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "2862  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "\n",
      "       14   15   16   17   18   19   20   21   22   23   24   25   26   27  \\\n",
      "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "2858  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "2859  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "2860  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "2861  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "2862  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "\n",
      "       28   29   30   31   32   33   34   35   36   37   38   39   40   41  \\\n",
      "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "2858  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "2859  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "2860  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "2861  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "2862  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "\n",
      "       42   43   44   45   46   47   48   49   50   51   52   53   54   55  \\\n",
      "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "2858  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "2859  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "2860  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "2861  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "2862  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "\n",
      "       56   57   58   59   60   61   62   63   64   65   66   67   68   69  \\\n",
      "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "2858  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "2859  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "2860  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "2861  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "2862  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "\n",
      "       70   71   72   73   74   75   76   77   78   79   80   81   82   83  \\\n",
      "0     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "1     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "2     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "3     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "4     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
      "2858  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "2859  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "2860  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "2861  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "2862  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "\n",
      "       84   85   86   87  \n",
      "0     0.0  0.0  0.0  0.0  \n",
      "1     0.0  0.0  0.0  0.0  \n",
      "2     0.0  0.0  0.0  0.0  \n",
      "3     0.0  0.0  0.0  0.0  \n",
      "4     0.0  0.0  0.0  0.0  \n",
      "...   ...  ...  ...  ...  \n",
      "2858  0.0  0.0  0.0  0.0  \n",
      "2859  0.0  0.0  0.0  0.0  \n",
      "2860  0.0  0.0  0.0  0.0  \n",
      "2861  0.0  0.0  0.0  0.0  \n",
      "2862  0.0  0.0  0.0  0.0  \n",
      "\n",
      "[2863 rows x 88 columns]\n"
     ]
    }
   ],
   "source": [
    "normal = get_dataset(\"./normal\")\n",
    "input = normal\n",
    "\n",
    "#convert to pandas dataframe\n",
    "df = pd.DataFrame(input)\n",
    "\n",
    "with pd.option_context('display.max_rows', 10, 'display.max_columns', None):  # more options can be specified also\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load all datasets\n",
    "Load all datasets\n",
    "Create numerical lables for each class, and a different set of labels for each subclass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./normal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dave/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:11: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./corrupt_0.1perc\n",
      "./corrupt_0.5perc\n",
      "./corrupt_1.0perc\n",
      "./delay_1_var_1\n",
      "./delay_5_var_2\n",
      "./delay_10_var_5\n",
      "./delay_25_var_20\n",
      "./drop_01_perc\n",
      "./drop_001_perc\n",
      "./drop_01_perc\n",
      "./drop_0005_perc\n",
      "./dup-1-p\n",
      "./dup_2perc\n"
     ]
    }
   ],
   "source": [
    "normal = get_dataset(\"./normal\")\n",
    "corr_01 = get_dataset(\"./corrupt_0.1perc\")\n",
    "corr_05 = get_dataset(\"./corrupt_0.5perc\")\n",
    "corr_10 = get_dataset(\"./corrupt_1.0perc\")\n",
    "delay_1_1 = get_dataset(\"./delay_1_var_1\")\n",
    "delay_5_2 = get_dataset(\"./delay_5_var_2\")\n",
    "delay_10_5 = get_dataset(\"./delay_10_var_5\")\n",
    "delay_25_20 = get_dataset(\"./delay_25_var_20\")\n",
    "drop_01 = get_dataset(\"./drop_01_perc\")\n",
    "drop_001 = get_dataset(\"./drop_001_perc\")\n",
    "drop_001_2 = get_dataset(\"./drop_01_perc\")\n",
    "drop_0005 = get_dataset(\"./drop_0005_perc\")\n",
    "dup_1 = get_dataset(\"./dup-1-p\")\n",
    "dup_2 = get_dataset(\"./dup_2perc\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['134.197.42.229', 38008, 5, ..., '-', 0.0, 0.0],\n",
       "       ['134.197.42.229', 38006, 9, ..., '-', 0.0, 0.0],\n",
       "       ['134.197.42.229', 43942, 10146, ..., '-', 0.0, 0.0],\n",
       "       ...,\n",
       "       ['134.197.42.229', 43720, 9, ..., '134.197.5.1',\n",
       "        1563302295355.0547, 1563302295358.2683],\n",
       "       ['134.197.42.229', 49656, 10255, ..., '134.197.5.1',\n",
       "        1563300835724.142, 1563300835725.48],\n",
       "       ['134.197.42.229', 49654, 16, ..., '134.197.5.1',\n",
       "        1563300835724.142, 1563300835725.48]], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '134.197.5.1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-9a081c1955c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                            dup_1, dup_2))\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mall_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    610\u001b[0m         \"\"\"\n\u001b[1;32m    611\u001b[0m         X = check_array(X, accept_sparse=('csr', 'csc'), copy=self.copy,\n\u001b[0;32m--> 612\u001b[0;31m                         warn_on_dtype=True, estimator=self, dtype=FLOAT_DTYPES)\n\u001b[0m\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m         \u001b[0;31m# Even in the case of `with_mean=False`, we update the mean anyway\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    431\u001b[0m                                       force_all_finite)\n\u001b[1;32m    432\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '134.197.5.1'"
     ]
    }
   ],
   "source": [
    "\n",
    "all_data = np.concatenate((normal,\n",
    "                           corr_01, corr_05, corr_10,\n",
    "                           delay_1_1, delay_5_2,delay_10_5,delay_25_20,\n",
    "                           drop_01, drop_001, drop_0005,\n",
    "                           dup_1, dup_2))\n",
    "\n",
    "all_data = StandardScaler().fit_transform(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_data = np.concatenate((normal,\n",
    "                           corr_01, corr_05, corr_10,\n",
    "                           delay_1_1, delay_5_2,delay_10_5,delay_25_20,\n",
    "                           drop_01, drop_001, drop_0005,\n",
    "                           dup_1, dup_2))\n",
    "\n",
    "all_data = StandardScaler().fit_transform(all_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
